{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations with Spark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_user(a, user):\n",
    "    return [Rating(user, item, ranking) for item, ranking in enumerate(a) if ranking != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_all(a):\n",
    "    return [expand_user(items, user) for user, items in enumerate(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we have ratings from eight users for six different movies: Titanic, Dirty Dancing, Die Hard, Terminator 2, Wayne's World, and Zoolander. Or in other words, two romantic films, two action films, and two comedies. Each row is a user, each column is a movie.\n",
    "\n",
    "### The ratings are constructed so that if a user has seen both movies in one of these pairs, their ratings for the two movies are similar.\n",
    "\n",
    "### There is no evidence in this data that anyone likes all three film genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawdata = [\n",
    "    [5,5,0,0,0,0],\n",
    "    [0,0,5,5,0,0],\n",
    "    [0,0,0,0,5,5],\n",
    "    [0,1,5,5,5,0],\n",
    "    [1,1,5,0,5,5],\n",
    "    [5,5,0,5,1,1],\n",
    "    [5,0,0,5,0,1],\n",
    "    [5,5,5,0,1,0]\n",
    "    ]\n",
    "list_of_ratings = expand_all(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=0, rating=5.0),\n",
       " Rating(user=0, product=1, rating=5.0),\n",
       " Rating(user=1, product=2, rating=5.0),\n",
       " Rating(user=1, product=3, rating=5.0),\n",
       " Rating(user=2, product=4, rating=5.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct an RDD of Ratings for every non-zero rating\n",
    "ratings = [val for sublist in list_of_ratings for val in sublist]\n",
    "ratingsRDD = sc.parallelize(ratings)\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank = 3\n",
    "numIterations = 5\n",
    "als_lambda = 0.1\n",
    "model = ALS.train(ratingsRDD, rank, numIterations, als_lambda, seed=4242, nonnegative=True)\n",
    "# there is also a trainImplicit method that one uses when\n",
    "# working with implicit ratings (it uses a different cost function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array('d', [0.0548248365521431, 2.1439249515533447, 0.1868879497051239])),\n",
       " (1, array('d', [1.116753339767456, 1.0875924825668335, 0.09992443770170212])),\n",
       " (2,\n",
       "  array('d', [1.4131990671157837, 0.17979858815670013, 0.8607355952262878])),\n",
       " (3,\n",
       "  array('d', [1.6533339023590088, 0.4600432813167572, 0.22208528220653534])),\n",
       " (4, array('d', [1.4229881763458252, 0.3441064953804016, 0.7850479483604431])),\n",
       " (5,\n",
       "  array('d', [0.12839074432849884, 2.1138052940368652, 0.011434999294579029])),\n",
       " (6, array('d', [0.16750510036945343, 2.0847885608673096, 0.0])),\n",
       " (7, array('d', [0.08329737186431885, 2.131591796875, 0.13894794881343842]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we see the model's vector of features for each user\n",
    "users = model.userFeatures().collect()\n",
    "sorted(users, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array('d', [0.14415070414543152, 2.2892513275146484, 0.10815300792455673])),\n",
       " (1,\n",
       "  array('d', [0.01397976279258728, 2.252133369445801, 0.24288420379161835])),\n",
       " (2, array('d', [2.3235769271850586, 2.1165413856506348, 1.0717264413833618])),\n",
       " (3, array('d', [2.2422642707824707, 2.17822265625, 0.2330417037010193])),\n",
       " (4, array('d', [2.7039616107940674, 0.3365514874458313, 1.2686758041381836])),\n",
       " (5,\n",
       "  array('d', [2.4558310508728027, 0.31472620368003845, 1.6110131740570068]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the features for the \"products\"\n",
    "products = model.productFeatures().collect()\n",
    "sorted(products, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=2, product=4, rating=4.9737419315999345),\n",
       " Rating(user=2, product=5, rating=4.913821860454357),\n",
       " Rating(user=2, product=2, rating=4.586700995228753)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend 3 items for user 2\n",
    "model.recommendProducts(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the original matrix side-by-side with the reconstructed matrix. The values that were originally non-zero should be closely approximated, and the values that were zero (empty) now have predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " original      reconstructed\n",
      "5 5 0 0 0 0     5 5 5 5 1 1  \n",
      "0 0 5 5 0 0     3 2 5 5 4 3  \n",
      "0 0 0 0 5 5     1 1 5 4 5 5  \n",
      "0 1 5 5 5 0     1 1 5 5 5 5  \n",
      "1 1 5 0 5 5     1 1 5 4 5 5  \n",
      "5 5 0 5 1 1     5 5 5 5 1 1  \n",
      "5 0 0 5 0 1     5 5 5 5 1 1  \n",
      "5 5 5 0 1 0     5 5 5 5 1 1  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\" original      reconstructed\")\n",
    "for user in range(0, len(rawdata)):\n",
    "    for product in range (0, len(rawdata[0])):\n",
    "        sys.stdout.write(\"%d \" % rawdata[user][product])\n",
    "    sys.stdout.write(\"    \")\n",
    "    for product in range (0, len(rawdata[0])):\n",
    "        sys.stdout.write(\"%0.0f \" % model.predict(user, product))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " original         errors        predictions\n",
      "5 5 0 0 0 0     - - - - - -     - - 5 5 1 1  \n",
      "0 0 5 5 0 0     - - - - - -     3 2 - - 4 3  \n",
      "0 0 0 0 5 5     - - - - - -     1 1 5 4 - -  \n",
      "0 1 5 5 5 0     - - - - - -     1 - - - - 5  \n",
      "1 1 5 0 5 5     - - - - - -     - - - 4 - -  \n",
      "5 5 0 5 1 1     - - - - - -     - - 5 - - -  \n",
      "5 0 0 5 0 1     - - - - - -     - 5 5 - 1 -  \n",
      "5 5 5 0 1 0     - - - - - -     - - - 5 - 1  \n"
     ]
    }
   ],
   "source": [
    "print(\" original         errors        predictions\")\n",
    "for user in range(0, len(rawdata)):\n",
    "    for product in range (0, len(rawdata[0])):\n",
    "        sys.stdout.write(\"%d \" % rawdata[user][product])\n",
    "    sys.stdout.write(\"    \")\n",
    "    for product in range (0, len(rawdata[0])):\n",
    "        if rawdata[user][product] != 0:\n",
    "            prediction = model.predict(user, product)\n",
    "            if rawdata[user][product] != round(prediction, 0):\n",
    "                sys.stdout.write(\"%0.0f \" % prediction)\n",
    "            else:\n",
    "                sys.stdout.write(\"- \")\n",
    "        else:\n",
    "            sys.stdout.write(\"- \")\n",
    "    sys.stdout.write(\"    \")\n",
    "    for product in range (0, len(rawdata[0])):\n",
    "        if rawdata[user][product] == 0:\n",
    "            prediction = model.predict(user, product)\n",
    "            sys.stdout.write(\"%0.0f \" % prediction)\n",
    "        else:\n",
    "            sys.stdout.write(\"- \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean squared error of the reconstructed matrix. This can be used to decide if the rank is sufficiently large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (1, 2), (1, 3), (2, 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalRDD = ratingsRDD.map(lambda p: (p[0], p[1]))\n",
    "evalRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 4.936098574134385),\n",
       " ((0, 1), 4.8745634940532785),\n",
       " ((1, 2), 5.0038884557410155),\n",
       " ((1, 3), 4.896361260535194),\n",
       " ((2, 4), 4.9737419315999345)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predictAll(evalRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 2), (5.0, 4.876094776769769)),\n",
       " ((5, 1), (5.0, 4.765143692061115)),\n",
       " ((4, 5), (5.0, 4.867640466522756)),\n",
       " ((3, 1), (1.0, 1.1131330479573036)),\n",
       " ((4, 4), (5.0, 4.9594862914862325))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsAndPreds = ratingsRDD.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "ratingsAndPreds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013936872542062233"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larger dataset we would separate the rating data into training and test sets, and see how well our predicted ratings match the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How does lambda affect the results?\n",
    "\n",
    "* try setting lambda to 0.01 (this is the default in some versions of spark)\n",
    "* can you get good results? what if you increase the rank?\n",
    "\n",
    "What happens as you increase (or decrease) the rank?\n",
    "\n",
    "How sensitive are the results to the random seed?\n",
    "\n",
    "What would happen if one movie was universally loved, or hated?\n",
    "\n",
    "What happens if you remove some of the rating data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
